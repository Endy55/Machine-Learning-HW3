---
title: $K$NN
author: "Endy Zarate"
date: "02/10/2025"

format: 
  html:  # You will quite likely want to change all but the last one, to taste
    theme: superhero  
    mainfont: monospace
    highlight-style: github
    title-block-banner: true
    embed-resources: true

---

**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](https://raw.githubusercontent.com/cd-public/D505/refs/heads/master/hws/src/knn.qmd) hosted on GitHub pages.

# 0. Quarto Type-setting

- This document is rendered with Quarto, and configured to embed an images using the `embed-resources` option in the header.
- If you wish to use a similar header, here's is the format specification for this document:

```email
format: 
  html:
    embed-resources: true
```

# 1. Setup

```{r}
library(tidyverse)
library(caret)
library(fastDummies)
wine <- readRDS(gzcon(url("https://github.com/cd-public/D505/raw/master/dat/pinot.rds")))
```

## 2. $K$NN Concepts

> <span style="color:red;font-weight:bold">TODO</span>: *KNN Concepts draws on the Gestalt principle of proximity where data points that are close together are perceived as similar. The algorithm attempts to predict categorization based on a certain number of neighboring points: k. If we chose k = 1, then the model will predict based on the closest point. This could be an issue if the data point is an outlier or an anomaly. If we chose a big number for k and the data set is skewed, this could result in the largest (most) category being predicted every time which hampers the accuracy of our predictions. It’s important to choose an optimal number where our accuracy is the highest.*

## 3. Feature Engineering

1. Create a version of the year column that is a *factor* (instead of numeric).
2. Create dummy variables that indicate the presence of "cherry", "chocolate" and "earth" in the description.
  - Take care to handle upper and lower case characters.
3. Create 3 new features that represent the interaction between *time* and the cherry, chocolate and earth inidicators.
4. Remove the description column from the data.

```{r}
win = wine %>%
  mutate(factor_year = as.factor(year),
         description = tolower(description))

win = win %>%
  mutate(cherry = str_detect(description, "cherry"),
         chocolate = str_detect(description, "chocolate"),
         earth = str_detect(description, "earth")) %>%
  select(-description)

win = win %>%
  mutate(timecherry = (year * cherry),
         timechocolate = (year * chocolate),
         timeearth = (year * earth))
  
```
## 4. Preprocessing

1. Preprocess the dataframe from the previous code block using BoxCox, centering and scaling of the numeric features
2. Create dummy variables for the `year` factor column

```{r}
win %>% 
  preProcess(method = c("BoxCox","center","scale")) %>% 
  predict(win) %>%
  head()

win = win %>% 
  dummy_cols(
    select_columns = "factor_year",
    remove_most_frequent_dummy = T, 
    remove_selected_columns = T)
```


## 5. Running $K$NN

1. Split the dataframe into an 80/20 training and test set
2. Use Caret to run a $K$NN model that uses our engineered features to predict province
  - use 5-fold cross validated subsampling 
  - allow Caret to try 15 different values for $K$
3. Display the confusion matrix on the test data


```{r}
library(class)

set.seed(505)
win_index = createDataPartition(win$province, p = 0.8, list = FALSE)
train = win[ win_index, ]
test = win[-win_index, ]

fit = knn(
  train = select(train,-province), 
  test = select(test,-province), 
  k=15, 
  cl = train$province)

confusionMatrix(fit,factor(test$province))$overall
```

## 6. Kappa

How do we determine whether a Kappa value represents a good, bad or some other outcome?

> <span style="color:red;font-weight:bold">TODO</span>: *The Kappa value is a measure of how well the model and the actual data points align. A Kappa value of 1 is complete agreement between model and data. A value of 0 is no agreement between model and data. We determine how “good” or “bad” a Kappa value is based on how close to 1 it is. A high Kappa value would mean our model has high reliability.*

## 7. Improvement

How can we interpret the confusion matrix, and how can we improve in our predictions?

> <span style="color:red;font-weight:bold">TODO</span>: *There are a couple ways to improve our predictions. One of those ways are to change and improve the features in a model. Some features have significant influence over predictions while others don't. Engineering features that can be better predictors can improve the reliability of our model. Another way to improve predictions is to find the optimal number for k. Having a large k can decrease our predictions if our data set is too big and skewed. Having a low k can be bad for outliers. Finding the optimal k can increase our model predictions and reliability.*
